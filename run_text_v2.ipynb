{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f8210\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "事件 A 出現了 19854 次\n",
      "事件 B 出現了 19759 次\n",
      "事件 C 出現了 60387 次\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 進行 10000 次試驗\n",
    "num_trials = 100000\n",
    "\n",
    "# 計數器\n",
    "count_a = 0\n",
    "count_b = 0 \n",
    "count_c = 0\n",
    "\n",
    "for i in range(num_trials):\n",
    "    # 生成一個 0-1 之間的隨機數\n",
    "    rand_num = random.random()\n",
    "    \n",
    "    # 根據概率判斷事件\n",
    "    if rand_num < 0.2:\n",
    "        count_a += 1\n",
    "    elif rand_num < 0.4:\n",
    "        count_b += 1\n",
    "    else:\n",
    "        count_c += 1\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"事件 A 出現了 {count_a} 次\")\n",
    "print(f\"事件 B 出現了 {count_b} 次\") \n",
    "print(f\"事件 C 出現了 {count_c} 次\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\f8210\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_dict = {\n",
    "'AAA': 0,\n",
    "'AA+': 1,\n",
    "'AA': 2,\n",
    "'AA-': 3,\n",
    "'A+': 4,\n",
    "'A': 5,\n",
    "'A-': 6,\n",
    "'B+': 7,\n",
    "'B': 8,\n",
    "'B-': 9,\n",
    "'BB+': 10,\n",
    "'BB': 11,\n",
    "'BB-': 12,\n",
    "'BBB+': 13,\n",
    "'BBB': 14,\n",
    "'BBB-': 15,\n",
    "'CCC+': 16,\n",
    "'CCC': 17,\n",
    "'CCC-': 18,\n",
    "'D': 19\n",
    "}\n",
    "\n",
    "inv_grade_dict = {v: k for k, v in grade_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # 去除 HTML 標籤\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # 去除數字\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 去除標點符號\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 去除非英文單字\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 去除換行符號\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    \n",
    "    # 統一為小寫\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 詞性還原\n",
    "    lemmatized_text = ' '.join([WordNetLemmatizer().lemmatize(w) for w in nltk.word_tokenize(text)])\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(year):\n",
    "\n",
    "    def judge(row):\n",
    "\n",
    "        rand_num = random.random()\n",
    "        \n",
    "        if row[\"year\"] > 2019:\n",
    "            \n",
    "            grade_num = row[\"grade_num\"]\n",
    "            if rand_num < 0.2:\n",
    "                grade_num += 1\n",
    "            elif rand_num < 0.4:\n",
    "                grade_num -= 1\n",
    "        else:\n",
    "            grade_num = row[\"grade_num\"]\n",
    "        \n",
    "        if grade_num < 0:\n",
    "            grade_num = 0\n",
    "        elif grade_num > 19:\n",
    "            grade_num = 19\n",
    "            \n",
    "        return grade_num\n",
    "        \n",
    "    df_rate = pd.read_excel(\"標準普爾最新信用評級.xls\", header=7)\n",
    "    df_rate[\"year\"] = df_rate[\"S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (Rating Date)\"].dt.year\n",
    "    feature_names = [col.replace('[', '').replace(']', '').replace('<', '') for col in pd.read_excel(f\"NEW財務數據/財務數據/2019財務數據.xls\", header=7).columns]    \n",
    "    df_rate[\"grade_num\"] = df_rate[\"S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)\"].map(grade_dict)\n",
    "    df_rate[\"grade_num\"] = df_rate.apply(judge, axis=1)\n",
    "    df_rate[\"S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)\"] = df_rate[\"grade_num\"].map(inv_grade_dict)\n",
    "    dict_rate = dict(zip(df_rate['Exchange:Ticker'], df_rate['S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)']))\n",
    "\n",
    "    df = pd.read_excel(f\"NEW財務數據/財務數據/{year}財務數據.xls\", header=7)\n",
    "    \n",
    "    # replace columns as 2019 columns\n",
    "    df.columns = feature_names\n",
    "    \n",
    "    # map Exchange:Ticker to credit rating\n",
    "    df[\"rate\"] = df[\"Exchange:Ticker\"].map(dict_rate)\n",
    "    df['Exchange:Ticker'] = df['Exchange:Ticker'].str.split(':').str[-1]\n",
    "    \n",
    "    # For each ticker in the 'Exchange:Ticker' column, search for a matching text file\n",
    "    for ticker in df['Exchange:Ticker']:\n",
    "        txt_files = glob.glob(os.path.join(f'NEW文字檔/10-K文字檔/{year}txt/', f\"{ticker}_*.txt\"))\n",
    "        if txt_files:\n",
    "            with open(txt_files[0], 'r') as f:\n",
    "                content = clean_text(f.read())\n",
    "            df.loc[df['Exchange:Ticker'] == ticker, 'text'] = content\n",
    "        else:\n",
    "            df.loc[df['Exchange:Ticker'] == ticker, 'text'] = np.nan    \n",
    "\n",
    "    # Drop the following columns\n",
    "    df = df.drop(columns=[\"Company Name\", \"Security Tickers\",\"Exchange:Ticker\"])    \n",
    "\n",
    "    # Create a rating map dictionary\n",
    "    rating_map = {\n",
    "        'AAA': 1, 'AA+': 1, 'AA': 1, 'AA-': 1, 'A+': 1, 'A': 1, 'A-': 1,\n",
    "        'BBB+': 2, 'BBB': 2, 'BBB-': 2,\n",
    "        'BB+': 3, 'BB': 3, 'BB-': 3,\n",
    "        'B+': 4, 'B': 4, 'B-': 4, 'CCC+': 4, 'CCC': 4, 'CCC-': 4, 'D': 4,\n",
    "        'NR': np.nan\n",
    "    }\n",
    "\n",
    "    # Map the credit rating values to numerical values \n",
    "    df['rate'] = df['rate'].map(lambda x: rating_map.get(x, x))\n",
    "    \n",
    "    # drop the nan in rate column\n",
    "    df = df.dropna(subset=['rate'])\n",
    "\n",
    "    # Replace '-' with NaN values in all columns\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace('-', np.nan)\n",
    "\n",
    "    # Replace 'NM' with NaN values\n",
    "    df = df.replace('NM', np.nan)\n",
    "\n",
    "    # Fill NaN values with the mean\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != 'object':\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for year in [2019,2020,2021,2022]:\n",
    "    # Concatenate the DataFrames\n",
    "    df = pd.concat([df, data_processing(year)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 500 rows with a fixed random state\n",
    "df = df.sample(n=100, random_state=42)\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分成文本和數值型\n",
    "df_numeric = df.drop(\"text\", axis=1)\n",
    "df_text = df[[\"text\",\"rate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 純數值型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "X = df_numeric.drop('rate', axis=1)\n",
    "y = le.fit_transform(df_numeric['rate'])\n",
    "\n",
    "# 對 X 資料標準化\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化後的數值\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc = accuracy_score(y_test, y_test_pred)                             # Calculate the accuracy score\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')                # Calculate the weighted F1-score\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')  # Calculate the weighted precision\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')        # Calculate the weighted recall\n",
    "cm = confusion_matrix(y_test, y_test_pred)                            # Calculate the confusion matrix\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 純文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training and test sets\n",
    "df_train, df_test = train_test_split(df_text, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the DistilBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data for the training and validation sets\n",
    "tokenized_train = tokenizer(df_train[\"text\"].values.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_test = tokenizer(df_test[\"text\"].values.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the tokenized text through the DistilBERT model to get the hidden states\n",
    "with torch.no_grad():\n",
    "    hidden_train = model(**tokenized_train)\n",
    "    hidden_test = model(**tokenized_test)\n",
    "\n",
    "# Get only the [CLS] token hidden states\n",
    "cls_train = hidden_train.last_hidden_state[:,0,:]\n",
    "cls_test = hidden_test.last_hidden_state[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Concatenate the [CLS] token hidden states and the general features for the training set\n",
    "x_train = cls_train\n",
    "y_train = df_train[\"rate\"]\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# Concatenate the [CLS] token hidden states and the general features for the validation set\n",
    "x_test = cls_test\n",
    "y_test = df_test[\"rate\"]\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Print the shapes of the input and target tensors\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier(objective='multi:softprob',    # Set the objective function for multi-class classification\n",
    "                            n_estimators=1000,           # Set the number of boosting iterations\n",
    "                            max_depth=7,                 # Set the maximum depth of the decision trees\n",
    "                            learning_rate=0.1,           # Set the learning rate for the boosting algorithm\n",
    "                            random_state=42)             # Set the random state for reproducibility\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc = accuracy_score(y_test, y_test_pred)                             # Calculate the accuracy score\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')                # Calculate the weighted F1-score\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')  # Calculate the weighted precision\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')        # Calculate the weighted recall\n",
    "cm = confusion_matrix(y_test, y_test_pred)                            # Calculate the confusion matrix\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 數值 + 文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Concatenate the [CLS] token hidden states and the general features for the training set\n",
    "x_train = torch.cat((cls_train, torch.from_numpy(X_train)), 1)\n",
    "y_train = df_train[\"rate\"]\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# Concatenate the [CLS] token hidden states and the general features for the validation set\n",
    "x_test = torch.cat((cls_test, torch.from_numpy(X_test)), 1)\n",
    "y_test = df_test[\"rate\"]\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# Print the shapes of the input and target tensors\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier(objective='multi:softprob',    # Set the objective function for multi-class classification\n",
    "                            n_estimators=1000,           # Set the number of boosting iterations\n",
    "                            max_depth=7,                 # Set the maximum depth of the decision trees\n",
    "                            learning_rate=0.1,           # Set the learning rate for the boosting algorithm\n",
    "                            random_state=42)             # Set the random state for reproducibility\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc = accuracy_score(y_test, y_test_pred)                             # Calculate the accuracy score\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')                # Calculate the weighted F1-score\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')  # Calculate the weighted precision\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')        # Calculate the weighted recall\n",
    "cm = confusion_matrix(y_test, y_test_pred)                            # Calculate the confusion matrix\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
